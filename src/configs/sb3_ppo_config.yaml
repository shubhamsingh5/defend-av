mode: train
model: ppo_sb3
scenario: 'columbia.yml'

# DQN
learning_rate: 0.0003
buffer_size: 100000
batch_size: 256
gamma: 0.99
hidden_size: 256
episodes: 10000
train_freq: 1024
min_samples: 1000
epsilon_start: 1.0
epsilon_end: 0.001
epsilon_decay: 0.999

# PPO parameters
use_lstm: false
learning_rate: 3e-4
n_steps: 2048
batch_size: 256
n_epochs: 10
gamma: 0.99
gae_lambda: 0.95
ent_coef: 0.01
total_timesteps: 200000
